<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="chapter-newton-secant" xmlns:xi="http://www.w3.org/2001/XInclude">

<title>Newton's method and secant method</title>


    <introduction>
        <p><term>Newton's method</term> of solving an equation <m>f(x)=0</m> is based on iterating a specific function <m>g</m> such that every root of <m>f</m> is an attracting (usually super-attracting) point for <m>g</m>. The formula for <m>g</m> is geometrically motivated: it comes from tangent line approximation to the graph of <m>f</m> and therefore involves its derivative. Since the derivative is not always known, we consider an alternative <term>secant method</term> which replaces the tangent line by a secant line.</p>
    </introduction>

<section xml:id="section-newton-method-intro">
<title>Newton's method</title>

<p>The idea of Newton's method is geometrically natural. We want to solve <m>f(x) = 0</m> but since <m>f</m> may be complicated, we replace it by its linear approximation (tangent line at a certain point). The resulting linear equation is easy to solve. Of course, its solution is probably not a solution of the original equation, but perhaps it is close to it. The process repeats, using the previously found solution as a new point at which to draw a tangent line. A static image of this process is below. <url href="https://www.intmath.com/applications-differentiation/newtons-method-interactive.php">Interactive visualization</url> is also available.</p> 

<figure xml:id="newton1-png"><image source="images/newton1.png" /> <caption>Two steps of Newton's method</caption></figure>

<p>Formally, the linear approximation to <m>f</m> at a point <m>x</m> is <m>f(x+h) \approx f(x) + f'(x)h</m>. Equating the right hand side to 0 we find <m>h = -f(x)/f'(x)</m> which means that the point where the tangent line at <m>x</m> meets the line <m>y=0</m> is <m>x + h = x - f(x)/f'(x)</m>. Therefore, we can express Newton's method as iteration of the function 
<me> g(x) = x  - \frac{f(x)}{f'(x)} = \frac{xf'(x) -f(x)}{f'(x)}</me>
Note that any point where <m>f(x) = 0</m> and <m>f'(x)\ne 0</m> is a fixed point of <m>g</m>. In fact, it is a super-attracting point of <m>g</m> because 
<me> g'(x) = 1  - \frac{f'(x)f'(x) - f(x)f''(x)}{(f'(x))^2} = \frac{f(x)f''(x)}{(f'(x))^2}</me>
which is zero when <m>f(x) = 0</m>. </p>

<example xml:id="example-newton-find-g">
    <title>Find and simplify the function <m>g</m> of Newton's method</title>
<statement><p>Given <m>f(x) = x^3 - x</m>, find and simplify the function <m>g(x) = x - f(x)/f'(x)</m>.</p> </statement>
<solution> <p> Bringing <m>g</m> to common denominator often helps:
<me> g(x) = \frac{xf'(x) -f(x)}{f'(x)} = \frac{x(3x^2-1) - (x^3-x)}{3x^2-1} = \frac{2x^3}{3x^2-1}
</me>
</p></solution></example>

<p>Once we have the function <m>g</m>, the rest of the process is the same as in <xref ref="chapter-fixed-point-iteration" />, in particular in <xref ref="example-solve-fixed-point-iteration" />. Changing the first two lines of that program to 
<cd>
g = @(x) 2*x^3/(3*x^2-1); 
x0 = 0.6;
</cd>
we get <q>Found x = 1 after 11 steps</q>.  If the starting point is different, the process may converge to a different root of the equation <m>x^3-x = 0</m>. For example with <c>x0 = 0.57</c> we get <q>Found x = -1 after 14 steps</q>. A small change of initial condition produced a rather different solution; we went from finding x = 1 to finding x = -1. The pattern of what starting points produce which solution can be very complicated: see the illustrations on Wikipedia page <url href="https://en.wikipedia.org/wiki/Newton_fractal">Newton fractal</url>.</p>
</section>

<section xml:id="section-newton-method-issues">
<title>Potential issues with Newton's method</title>

<p>First potential issue: we might not have a formula for the derivative of function <m>f</m>. This can be overcome by using numeric differentiation, which is covered later in the course. </p>

<p>The points where <m>f'(x)=0</m> are problematic for Newton's method, for two reasons. One is that <m>g</m> is not defined at such a point because of division by zero. So, if we happen to arrive at such a point in the process of iteration, the process has to be stopped and restarted again with a different initial value. Even if the derivative is not exactly zero but is very close to it, the value of <m>g</m> can be huge, sending the search to a location far from the actual solutions. </p>

<p>The second reason the zeros of derivative are problematic is that a point where <m>f(x) = f'(x) = 0</m> is not a superattracting fixed point of <m>g</m>, even if we manage to extend the definition of <m>g</m> to such a point by canceling out the zero in the denominator. For example, let <m>f(x) = x^p</m> where <m>p \gt 1</m>. Clearly, <m>f(0)=f'(0)=0</m>. Computing
<me> g(x) = \frac{xf'(x) -f(x)}{f'(x)} = \frac{px^p - x^p}{px^{p-1}} = \frac{p-1}{p}x 
</me>
we see that <m>g</m> can be extended to <m>g(0)=0</m>. Since <m>|g'(0)| = (p-1)/p \lt 1</m>, this is an attracting point. But it is not a super-attracting one, which means the speed of convergence of Newton's method to such <term>multiple roots</term> is slow: linear instead of quadratic. Here is an example of what happens with <m>f(x) = x^3</m>: the points approach the root <m>x=0</m> quite slowly, with each step being <m>1/3</m> of the distance to the root. This is even slower than bisection.</p>

<figure xml:id="newton3-png"><image source="images/newton3.png" /> <caption>Newton's method converges slowly to a multiple root</caption></figure>

<p>A way to improve convergence in this case is presented in <xref ref="section-relaxation-parameter" />. 	Fortunately, a multiple root is a somewhat exceptional case and in practice we find Newton's method converging at quadratic rate. </p>

<p>But the most serious issue is that the method may fail to converge at all, either because the iteration of <m>g</m> gets stuck in a loop or because it tends to infinity. A typical example is <m>f(x) = \tan^{-1}x</m> (note that Matlab notation for arctangent function is <c>atan</c>). Here <m>g(x) = x - (x^2+1)\tan^{-1}(x)</m>, which is <c>g = @(x) x - (x^2+1)*atan(x);</c> in Matlab notation. With the initial value <c>x0 = 1</c> the process quickly converges to 0. With the initial value <c>x0 = 2</c> (or even 1.5) it fails to converge. The problem can be seen numerically by computing a few iterations in Matlab console.</p> 
<pre>
>> g(1.5)
ans = -1.6941
>> g(ans)
ans =  2.3211
>> g(ans)
ans = -5.1141
>> g(ans)
ans =  32.296
</pre>
<p>This also illustrates the use of <c>ans</c>, the variable that has the result of previous computation. Geometrically, Newton's method overshoots the target because the tangent lines have small slope.</p>

<figure xml:id="newton2-png"><image source="images/newton2.png" /> <caption>Newton's method overshoots and fails to converge </caption></figure>
</section> 


<section xml:id="section-secant-method">
<title>The secant method</title>

<p>The secant method is a close relative of Newton's method. The only difference is that instead of drawing a tangent line and finding where it crosses the line <m>y=0</m> we do the same for a secant line. The slope of a secant line is computed as <m>(f(x)-f(p))/(x-p)</m> where <m>x, p</m> are two distinct points on the x-axis. The benefit is that we do not need the derivative of <m>f</m>, we only use the function itself. The drawback is that we have to keep track not of just one point, but of two. This makes the algorithm slightly more complicated: it is not just fixed-point iteration of some function <m>g</m> as in Newton's method. Also, the analysis of its speed of convergence is more complicated: generally, it is between linear and quadratic; faster than bisection but slower than Newton. </p>

<example xml:id="example-secant-method">
    <title>Implement the secant method</title>
<statement><p>Write a script that starts with the definition of a function <m>f</m> and two initial points, for example 
<cd>
f = @(x) x^3 - x;
x = 5;
p = 7;
</cd>
and proceeds to find a root of <m>f</m> using the secant method: the next point to be computed will be 
<me>x - f(x) \frac{x-p}{f(x)-f(p)}</me>
where the fraction represents division by the slope of secant line. </p></statement>
<solution> 
<pre>
f = @(x) x^3 - x;
x = 5;
p = 7;

for k = 1:1000
    x1 = x - f(x)*(x-p)/(f(x)-f(p));
    if abs(x1-x) &lt; 100*eps(x)
        break
    end
    p = x;
    x = x1;
end

if k &lt; 1000
    fprintf('Found x = %.12g after %d steps\n', x, k);
else
    disp('Failed to converge')
end
</pre>
<p>This script follows the structure of <xref ref="example-solve-fixed-point-iteration" /> except that we compute "new x", called x1, on the basis of two previous values (called <c>x</c> and <c>p</c>). After the computation, <c>x</c> and <c>p</c> are replaced by <c>x1</c> and <c>x</c>: the newly computed point becomes one of the two points through which we draw next secant line.</p>
</solution></example>
</section>



<section xml:id="section-relaxation-parameter">
<title>The relaxation parameter</title>

<p>When faced with an iterative method which moves either too slowly, undershooting the target (<xref ref="newton3-png" />) or too quickly, overshooting it (<xref ref="newton2-png" />), we can try and fix the issue by introducing a <term>relaxation parameter</term>. This is a fixed positive number <m>\omega \gt 0</m> by which we multiply the step size <m>h</m> obtained from Newton's method (or secant method, etc). That is, instead of iterating the function 
<me>g(x) = x - \frac{f(x)}{f'(x)}</me>
we iterate the function 
<me>g(x) = x - \omega \frac{f(x)}{f'(x)}</me>
If <m>\omega > 1</m>, this is <term>over-relaxation</term>, meant to make the process move faster. If <m>\omega \lt 1</m>, this is <term>under-relaxation</term>, meant to slow down the process so it does not overshoot and run off in a wrong direction. </p> 

<p>How to determine an appropriate value of <m>\omega</m>? Consider the case of a multiple root: <m>f(x) = x^p</m> with <m> p > 1</m>.  Here 
<me>g(x) = x - \omega \frac{f(x)}{f'(x)} = x - \omega \frac{x^p}{px^{p-1}} = (1- \omega/p)x</me>
so the best value to use <m>\omega = p</m>, which makes <m>g(x)=0</m>, the solution is found in one step. 
So, for a double root, when the graph of <m>f</m> looks like a parabola near the root, we should use over-relaxation with <m>\omega = 2</m>; for triple roots, <m>\omega = 3</m>, and so on. Over-relaxation needs to be done very carefully: in these examples, any value of <m>\omega</m> other than the correct one will fail to achieve the desired speed up of convergence. 
 </p>

<p>Similarly, a function shaped like <m>f(x) = x^{1/3}</m> may need under-relaxation, in this case the previous paragraph suggests <m>\omega = 1/3</m>. In general, under-relaxation is a less delicate issue than over-relaxation: seeing the failure to converge, one can simply try <m>\omega = 0.1</m>, and if that does not help, maybe <m>\omega = 0.01</m>. The convergence, if it is achieved, will usually be slow (linear).</p>
</section>

<section xml:id="examples-newton-secant">

<title>Examples and questions</title>  

<p> These are additional examples for reviewing the topic we have covered. When reading each example, try to find your own solution before clicking <q>Answer</q>. There are also questions for reviewing the concepts of this section. </p>

<example xml:id="example-newton-discontinuous">
   <title> Newton's method with a discontinuous function </title>
<statement><p>Recall from <xref ref="example-bisection-discontinuity" /> that the bisection method had a difficulty with the equation <m>\tan x = 0</m>, mistaking a discontinuity of this function for its root. How does Newton's method behave here, and why? </p>
</statement>
<solution><p>Newton's method has no issues with this function: if the starting point is close to a discontinuity, it will move away from it, toward the nearest zero. It will never overshoot the zero because of the concavity of the function: it is concave away from the x-axis, which means the tangent lines are between the graph and the x-axis. </p> 	
<figure xml:id="newton4-png"><image source="images/newton4.png" /> <caption>Newton's method works fine for the tangent function </caption></figure>
<p>Generally, vertical asymptotes are not a problem for Newton's method: horizontal asymptotes (and horizontal tangent lines) are. </p> 
</solution>
</example>

<example xml:id="example-secant-speed">
   <title> Rate of convergence of the secant method </title>
<statement><p>It is difficult to analyze the convergence of secant method in general. A representative example where such analysis can be done is <m>f(x) = x + x^2</m>. Find a simplified form of the function 
<m>x - \dfrac{x-p}{f(x)-f(p)} f(x)</m> and try to justify the statement that the rate of convergence is faster than linear, assuming that <m>x</m> and <m>p</m> converge to 0.</p>
</statement>
<solution><p>Simplification shows that 
<md><mrow>x - \frac{x-p}{f(x)-f(p)} f(x)
	\amp = x - \frac{x-p}{(x-p) + (x^2-p^2)}(x-x^2) </mrow>
	<mrow> \amp = x - \frac{1}{1 + x +p}(x-x^2)  </mrow>
    <mrow> \amp = \frac{x + x^2 + px - (x+x^2)}{1 - x -p} </mrow>
    <mrow> \amp = \frac{px}{1 - x -p} </mrow>
</md>
Since <m>x, p \to 0</m>, the denominator is close to 1. Therefore, the method goes from <m>x</m> to approximately <m>px</m> where <p>p</p> is the point preceding <m>x</m>. For comparison, linear convergence would mean going from <m>x</m> to approximately <m>cx</m> where <m>c</m> is a fixed number. 
Since the factor <p>p</p> itself goes to zero, the secant method has faster than linear rate of convergence. 
</p></solution></example>

 
<question><title> Equation with no solution </title>
<p>What happens if either Newton's method or secant method is applied to the equation <m>|x|+1 = 0</m> (which, of course, has no solutions)? 
</p></question> 

<question><title> Another equation with no solution </title>
<p>What happens if Newton's method is applied to the equation <m>e^x = 0</m> (which has no solutions)? 
</p></question> 

</section>


<exercises xml:id="exercises-newton-secant">
    <title>Homework</title>

    <exercise number="1">
        <statement>
<p> (Theoretical problem.) Suppose that Newton's method is used to solve <m>f(x)=0</m> with <m>f(x) = e^x + e^{-x} - 2</m>. Show that the function <m>g(x)=x-f(x)/f'(x)</m> (extended by <m>g(0)=0</m>) has a nonzero derivative at <m>0</m> by computing the limit of <m>g(x)/x</m> as <m>x\to 0</m>. </p>
</statement>
<hint><p>Hint: To simplify <m>f(x)/f'(x)</m>, you can multiply the numerator and denominator by <m>e^x</m> and recognize that they both can be factored.</p> </hint>
</exercise>

    <exercise number="2">
        <statement>
<p>(Theoretical problem.) Show that if we use <m>g(x)=x-2f(x)/f'(x)</m> in the previous problem, this function <m>g</m> has <m>g'(0)=0</m>, which makes <m>0</m> a superattracting fixed point. </p></statement></exercise>

    <exercise number="3">
        <statement>
<p>Write a script that attempts to solve the equation <m>\tan x - 1 = 0</m> using Newton's method. Similarly to <xref ref="example-secant-method" /> and <xref ref="example-solve-fixed-point-iteration" />, the script should show the number of steps it took to find a solution, or report that it failed to converge. Allow the algorithm up to 10000 steps before giving up. As the initial point <c>x0</c>, use the number formed by the first two digits of your SUID. (Note: it is expected that your script will fail to converge.)</p>
 
<p>Introduce a relaxation parameter <m>\omega</m> in your script to help it converge. What value of <m>\omega</m> achieves convergence? Be careful: <m>\omega</m> should be small, but if it is too small, the algorithm will fail to converge because it takes too long.</p> 

<p>When submitting homework on Blackboard, comment on what values of <m>\omega</m> you found to be too large for convergence, what value was too small for convergence, and what value achieved convergence.</p>
</statement>
</exercise>
 
</exercises>

</chapter>